spark: 
  threads: 8
  format: "json"
  # none, bzip2, gzip, lz4, snappy and deflate
  compression: "gzip"
  normalize_schema: "true"
  timestamp_format: "yyyy-MM-dd HH:mm:ss"
  properties:
    spark.ui.showConsoleProgress: "false"
    spark.default.parallelism: "8"
    spark.eventLog.enabled: "false"
    spark.eventLog.dir: "spark_log"
    spark.driver.extraClassPath: "sqljdbc42.jar:gcs-connector-hadoop3-latest.jar:spark-3.1-bigquery-0.28.0-preview.jar"
    spark.sql.session.timeZone: "America/Los_Angeles"
    spark.sql.jsonGenerator.ignoreNullFields": "false"
    spark.sql.debug.maxToStringFields: "25"
    #spark.dynamicAllocation.enabled: "true"
    #spark.dynamicAllocation.minExecutors: "1"
    #spark.dynamicAllocation.maxExecutors: "4"
    spark.hadoop.google.cloud.auth.service.account.enable: "true"
    spark.hadoop.google.cloud.auth.service.account.json.keyfile: "key.json"
    spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version: "2"
    spark.executor.memory: "8g"
    spark.driver.memory: "16g"
    # Change to G1GC prevents GC slowdowns entirely
    spark.executor.defaultJavaOptions: "-XX:+UseG1GC"
    spark.driver.defaultJavaOptions: "-XX:+UseG1GC"
    spark.storage.memoryFraction: "0"
    spark.sql.debug.maxToStringFields: "500"
sqlalchemy:
  url: "mssql+pyodbc://USERNAME:PASSWORD@DBHOSTNAME:PORT/DATABASE?driver=ODBC+Driver+18+for+SQL+Server&TrustServerCertificate=yes&Encrypt=yes"
  pool_size: 20
  max_overflow: 20
  connect_args:
    connect_timeout: 60
jdbc:
  url: "jdbc:sqlserver://HOSTNAME:PORT;databaseName=DATABASE;"
  properties:
    user: USERNAME
    password: "PASSWORD"
    driver: "com.microsoft.sqlserver.jdbc.SQLServerDriver"
    fetchsize: "1000"
schema: dbo
# Python thread pool -- used for introspection and launching spark Jobs
job_threads: 32 
# Your target file size for Spark output files
target_partition_size_bytes: 52428800
# Introspection expires after this many seconds since last
introspection_expire_s: 604800
tinydb_database_file: tinydb.json

# Controls parallelism of SQL introspection workers (one SQL connection per worker)
# note: introspect_workers + spark.threads = total parallel SQL queries
introspect_workers: 8
# No more than this number of Spark jobs will be in 'running' state.
extract_workers: 4
# Controls parallelism of BigQuery load operations 
load_workers: 8

